{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import  SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=\"data_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_schema=spark.read.csv(data,sep='\\t',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+----------+\n",
      "|  date_col| timestamp_col|quotes_col|\n",
      "+----------+--------------+----------+\n",
      "|11-10-2019|10/10/19 08:00|    “data”|\n",
      "|11-10-2018|11/10/19 08:00|    “data”|\n",
      "|11-10-2017|12/10/19 08:00|    “data”|\n",
      "|11-10-2016|13/10/19 08:00|    “data”|\n",
      "|11-10-2015|14/10/19 08:00|    “data”|\n",
      "+----------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_without_schema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema=\"date_col DATE,time_col TIMESTAMP,quotes STRING,corrupt_rec STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv(data,sep='\\t',header=True,schema=schema,columnNameOfCorruptRecord=\"corrupt_rec\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('date_col', 'date'),\n",
       " ('time_col', 'timestamp'),\n",
       " ('quotes', 'string'),\n",
       " ('corrupt_rec', 'string')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+--------------------------------+\n",
      "|date_col|time_col|quotes|corrupt_rec                     |\n",
      "+--------+--------+------+--------------------------------+\n",
      "|null    |null    |null  |11-10-2019\t10/10/19 08:00\t“data”|\n",
      "|null    |null    |null  |11-10-2018\t11/10/19 08:00\t“data”|\n",
      "|null    |null    |null  |11-10-2017\t12/10/19 08:00\t“data”|\n",
      "|null    |null    |null  |11-10-2016\t13/10/19 08:00\t“data”|\n",
      "|null    |null    |null  |11-10-2015\t14/10/19 08:00\t“data”|\n",
      "+--------+--------+------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleansed_date_time=df=spark.read.csv(data,sep='\\t',header=True,schema=schema,\n",
    "                                        dateFormat=\"dd-MM-YYYY\",timestampFormat=\"dd/MM/YY HH:mm\"\n",
    "                                        ,columnNameOfCorruptRecord=\"corrupt_rec\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------+-----------+\n",
      "|  date_col|           time_col|quotes|corrupt_rec|\n",
      "+----------+-------------------+------+-----------+\n",
      "|2019-10-11|2019-10-10 08:00:00|“data”|       null|\n",
      "|2018-10-11|2019-10-11 08:00:00|“data”|       null|\n",
      "|2017-10-11|2019-10-12 08:00:00|“data”|       null|\n",
      "|2016-10-11|2019-10-13 08:00:00|“data”|       null|\n",
      "|2015-10-11|2019-10-14 08:00:00|“data”|       null|\n",
      "+----------+-------------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleansed_date_time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quotes_data=spark.read.csv(data,sep='\\t',header=True,schema=schema,\n",
    "                                        dateFormat=\"dd-MM-YYYY\",timestampFormat=\"dd/MM/YY HH:mm\"\n",
    "                                        ,columnNameOfCorruptRecord=\"corrupt_rec\",quote=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------+-----------+\n",
      "|  date_col|           time_col|quotes|corrupt_rec|\n",
      "+----------+-------------------+------+-----------+\n",
      "|2019-10-11|2019-10-10 08:00:00|“data”|       null|\n",
      "|2018-10-11|2019-10-11 08:00:00|“data”|       null|\n",
      "|2017-10-11|2019-10-12 08:00:00|“data”|       null|\n",
      "|2016-10-11|2019-10-13 08:00:00|“data”|       null|\n",
      "|2015-10-11|2019-10-14 08:00:00|“data”|       null|\n",
      "+----------+-------------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_quotes_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compressed_data=spark.read.option(\"compression\",\"gzip\").csv(\"compressed.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|_c0|_c1|_c2|_c3|\n",
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_compressed_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compressed_data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
